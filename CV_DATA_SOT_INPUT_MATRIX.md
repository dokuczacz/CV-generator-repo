# CV Data — Single Source of Truth (SoT) Input Matrix

## Overview
This matrix defines WHERE each CV section's data comes from, HOW it's transformed, and WHICH stages produce it.

---

## Section 1: Header (Name, Contact, Photo)

| Field | Data Source | Stage(s) | Input Format | Transformations | Storage Location |
|-------|---|---|---|---|---|
| `full_name` | User input | contact | String | None | `cv_data.full_name` |
| `address_lines` | User input | contact | Array[string] | Trimmed, split by line | `cv_data.address_lines` |
| `phone` | User input | contact | String | Trimmed | `cv_data.phone` |
| `email` | User input | contact | String | Trimmed, validated | `cv_data.email` |
| `birth_date` | User input (opt) | contact | "DD Month YYYY" | Trimmed | `cv_data.birth_date` |
| `nationality` | User input (opt) | contact | String | Trimmed | `cv_data.nationality` |
| `photo_url` | User input (opt) | contact | Base64 data URI | ≤32KB | `cv_data.photo_url` |

---

## Section 2: Work Experience

| Field | Data Source | Stage(s) | Input Format | Transformations | Storage Location |
|-------|---|---|---|---|---|
| `work_experience[].date_range` | DOCX extract + user edit | work_experience | "YYYY-MM – YYYY-MM" | Parsed, normalized | `cv_data.work_experience[].date_range` |
| `work_experience[].employer` | DOCX extract + user edit | work_experience | String | Trimmed | `cv_data.work_experience[].employer` |
| `work_experience[].title` | DOCX extract + user edit | work_experience | String | Trimmed | `cv_data.work_experience[].title` |
| `work_experience[].location` | DOCX extract + user edit | work_experience | String | Trimmed | `cv_data.work_experience[].location` |
| `work_experience[].bullets` | DOCX extract initially, then AI-generated in work_tailor_run | work_experience → work_tailor_run | Array[string] | Generated by OpenAI based on tailoring context, 3–4 bullets | `cv_data.work_experience[].bullets` |
| **Tailoring Context Object** | User input from step 4 work_experience stage | work_notes_edit → work_tailor_feedback → work_tailor_run | Structured object: { notes, job_summary, selected_role_index } | Saved once, reused by ALL downstream AI tasks | `meta.work_tailoring_notes` (string), `meta.job_reference` (dict), `meta.selected_work_role_index` (int) |

---

## Section 3: IT & AI Skills

| Field | Data Source | Stage(s) | Input Format | Transformations | Storage Location |
|-------|---|---|---|---|---|
| `it_ai_skills[]` | FÄHIGKEITEN & KOMPETENZEN (DOCX) + work tailoring context | skills_tailor_run | Array[string] | Ranked by job relevance, deduplicated, 5–10 max | `cv_data.it_ai_skills` |
| **Input #1: Candidate skills** | DOCX extract: section "FÄHIGKEITEN & KOMPETENZEN" | - | Array[string] | Extracted from DOCX | `docx_prefill_unconfirmed.it_ai_skills` |
| **Input #2: Tailoring context (reused from Step 4)** | Same object saved in step 4 | work_experience stage → skills_tailor_run | Object: { work_tailoring_notes, job_reference, selected_work_role_index } | Passed as-is to AI ranking task | `meta.work_tailoring_notes` + `meta.job_reference` + `meta.selected_work_role_index` |
| **Input #3: Job summary** | Parsed job posting | job_posting stage | Text block + structured dict | Extracted from URL or pasted text | `meta.job_reference` + `meta.job_posting_text` |
| **AI Operation** | OpenAI API call (task: it_ai_skills) | skills_tailor_run | Job context + candidate list + tailoring notes | Relevance ranking, filtering, reordering | Result → `cv_data.it_ai_skills` |

---

## Section 4: Technical & Operational Skills

| Field | Data Source | Stage(s) | Input Format | Transformations | Storage Location |
|-------|---|---|---|---|---|
| `technical_operational_skills[]` | FÄHIGKEITEN & KOMPETENZEN (DOCX) + work tailoring context | tech_ops_tailor_run | Array[string] | Ranked by job relevance, deduplicated | `cv_data.technical_operational_skills` |
| **Input #1: Candidate skills** | DOCX extract or manual entry | - | Array[string] | Extracted from DOCX or user-added | `docx_prefill_unconfirmed.technical_operational_skills` |
| **Input #2: Tailoring context (reused from Step 4)** | Same object saved in step 4 | work_experience stage → tech_ops_tailor_run | Object: { work_tailoring_notes, job_reference, selected_work_role_index } | Passed as-is to AI ranking task | `meta.work_tailoring_notes` + `meta.job_reference` + `meta.selected_work_role_index` |
| **Input #3: Job summary** | Parsed job posting | job_posting stage | Text block + structured dict | Same as IT/AI skills | `meta.job_reference` + `meta.job_posting_text` |
| **AI Operation** | OpenAI API call (task: technical_operational_skills) | tech_ops_tailor_run | Job context + candidate list + tailoring notes | Relevance ranking, filtering | Result → `cv_data.technical_operational_skills` |

---

## Section 5: Education

| Field | Data Source | Stage(s) | Input Format | Transformations | Storage Location |
|-------|---|---|---|---|---|
| `education[].date_range` | DOCX extract + user edit | education | "YYYY – YYYY" or "YYYY" | Parsed, normalized | `cv_data.education[].date_range` |
| `education[].institution` | DOCX extract + user edit | education | String | Trimmed | `cv_data.education[].institution` |
| `education[].title` | DOCX extract + user edit | education | String (degree name) | Trimmed | `cv_data.education[].title` |
| `education[].specialization` | DOCX extract + user edit (opt) | education | String | Trimmed | `cv_data.education[].specialization` |
| `education[].details[]` | DOCX extract + user edit (opt) | education | Array[string] | Thesis, GPA, honors | `cv_data.education[].details` |

---

## Section 6: Language Skills

| Field | Data Source | Stage(s) | Input Format | Transformations | Storage Location |
|-------|---|---|---|---|---|
| `languages[].name` or `.language` | DOCX extract | - | String | Language name (English, German, etc.) | `cv_data.languages[].name` or `.language` |
| `languages[].level` | DOCX extract | - | String | Proficiency level (Native, Fluent, B2, etc.) | `cv_data.languages[].level` |

---

## Section 7: Interests

| Field | Data Source | Stage(s) | Input Format | Transformations | Storage Location |
|-------|---|---|---|---|---|
| `interests` | DOCX extract | - | String | Paragraph or comma-separated list | `cv_data.interests` |

---

## Section 8: References

| Field | Data Source | Stage(s) | Input Format | Transformations | Storage Location |
|-------|---|---|---|---|---|
| `references` | DOCX extract | - | String | Contact names or "On request" | `cv_data.references` |

---

## Critical Dependencies & Data Flow

### Dependency Graph

```
DOCX Upload
    ↓
docx_prefill_unconfirmed (raw extract)
    ├── it_ai_skills
    ├── technical_operational_skills
    ├── work_experience
    ├── education
    ├── languages
    └── ... (other fields)

User confirms import (CONFIRM_IMPORT_PREFILL_YES)
    ↓
cv_data (canonical, confirmed)

Job Posting (pasted or URL)
    ↓
meta.job_reference (parsed: title, company, location, must-haves, keywords)
meta.job_posting_text (raw text)

═══════════════════════════════════════════════════════════════════════
STEP 4: Work Experience Tailoring (CRITICAL: SAVE AS OBJECT)
═══════════════════════════════════════════════════════════════════════

work_notes_edit stage (user enters notes)
    ↓
work_tailor_feedback stage (preview)
    ↓
work_tailor_run stage
    ├── Save to: meta.work_tailoring_notes (string: user context)
    ├── Save to: meta.selected_work_role_index (int: which role was focused)
    ├── Use: meta.job_reference (dict: job summary from step 2)
    └── Generate: work_experience[].bullets (by OpenAI)
    ↓
TAILORING CONTEXT OBJECT CREATED:
    {
        "work_tailoring_notes": "user notes from step 4",
        "selected_work_role_index": 0,
        "job_reference": { title, company, must_haves, keywords... },
        "job_posting_text": "raw posting"
    }

═══════════════════════════════════════════════════════════════════════
STEP 5a: Technical Projects (NOW DELETED – Skip this)
═══════════════════════════════════════════════════════════════════════

═══════════════════════════════════════════════════════════════════════
STEP 5b: IT & AI Skills Tailoring (REUSE TAILORING CONTEXT OBJECT)
═══════════════════════════════════════════════════════════════════════

skills_tailor_run stage
    ├── Input: it_ai_skills[] from (cv_data OR docx_prefill_unconfirmed)
    ├── Input: TAILORING CONTEXT OBJECT (from step 4) ← REUSED HERE
    │   ├── work_tailoring_notes
    │   ├── selected_work_role_index
    │   ├── job_reference
    │   └── job_posting_text
    └── Output: RANKED it_ai_skills[] → cv_data.it_ai_skills

═══════════════════════════════════════════════════════════════════════
STEP 5c: Technical & Operational Skills (REUSE TAILORING CONTEXT OBJECT)
═══════════════════════════════════════════════════════════════════════

tech_ops_tailor_run stage
    ├── Input: technical_operational_skills[] from (cv_data OR docx_prefill_unconfirmed)
    ├── Input: TAILORING CONTEXT OBJECT (from step 4) ← REUSED HERE
    │   ├── work_tailoring_notes
    │   ├── selected_work_role_index
    │   ├── job_reference
    │   └── job_posting_text
    └── Output: RANKED technical_operational_skills[] → cv_data.technical_operational_skills

═══════════════════════════════════════════════════════════════════════
```

---

## Tailoring Context Object Schema (SoT)

Once saved in Step 4, this object is reused by ALL downstream AI tasks:

```python
class JobReferenceObject:
    """Parsed job posting – immutable once created, reused across all AI tasks"""
    title: str  # Job title
    company: str  # Company name
    location: str  # Job location
    must_haves: list[str]  # Required qualifications
    keywords: list[str]  # Key technical skills / competencies
    raw_text: str  # Original posting for context

class TailoringContextObject:
    """Complete tailoring context – created once in Step 4, reused in Steps 5b, 5c"""
    work_tailoring_notes: str  # User's free-text context for profile tailoring
    job_reference: JobReferenceObject  # Parsed job posting (structured, immutable)
```

**Storage in `meta` (session metadata):**
```json
{
    "job_posting_url": "https://...",
    "job_posting_text": "Raw posting text...",
    "job_reference": {
        "title": "Senior Python Engineer",
        "company": "TechCorp",
        "location": "Zurich, CH",
        "must_haves": ["5+ years Python", "PostgreSQL", "AWS"],
        "keywords": ["FastAPI", "Docker", "CI/CD", "Kubernetes"]
    },
    "work_tailoring_notes": "String from user textarea in step 4"
}
```

**Reused by:**
1. `work_tailor_run` (step 4 → work bullet generation)
2. `skills_tailor_run` (step 5b → IT/AI skills ranking)
3. `tech_ops_tailor_run` (step 5c → technical skills ranking)

**Key principles:**
- ✅ Parse job posting ONCE (in job_posting_paste stage)
- ✅ Store as structured object in `meta.job_reference`
- ✅ Reuse same object in all downstream AI tasks
- ✅ Never re-parse or duplicate
- ✅ Object is immutable within a session

---

## Key Rules & Validations

| Rule | Stage(s) | Enforcement |
|------|---|---|
| **Tailoring Context Object is immutable** | Step 4 → Steps 5b, 5c | Once saved in step 4 (work_tailor_run), use same object in all downstream AI tasks. Do NOT modify between stages. |
| **No AI without job context** | skills_tailor_run, tech_ops_tailor_run | If no job_reference AND no job_posting_text → button grayed, auto-skip |
| **Skills come from DOCX** | it_ai_skills, technical_operational_skills | Cannot add skills not in candidate list (security + quality) |
| **Tailoring notes are optional** | work_experience | If empty, use job summary alone |
| **Work context applies to both skills sections** | work_experience → skills_tailor_run + tech_ops_tailor_run | Reuse same `work_tailoring_notes` + `job_reference` object in both ranking tasks |
| **Fallback to docx_prefill** | All stages | If cv_data field empty, check docx_prefill_unconfirmed first |
| **No removal of sections** | Template | All 8 sections must render; empty sections show "(none)" |
| **Object storage in metadata** | Step 4+ | `meta.work_tailoring_notes`, `meta.selected_work_role_index`, `meta.job_reference`, `meta.job_posting_text` must persist across session |

---

## Job Posting Parsing Pipeline (Python Object Pattern)

### Stage: `job_posting_paste` 
**Input:** Raw job posting text (pasted by user)
**Processing:**
1. Parse text → extract title, company, location, must-haves, keywords
2. Create `JobReferenceObject` (structured, immutable)
3. Store in `meta.job_reference` (ONE TIME ONLY)

**Output:** `meta.job_reference` object (ready for reuse)

```python
# Example: What happens in job_posting_paste stage

def parse_job_posting(raw_text: str, url: str = "") -> dict:
    """Parse job posting → JobReferenceObject"""
    job_ref = {
        "title": extract_job_title(raw_text),
        "company": extract_company(raw_text),
        "location": extract_location(raw_text),
        "must_haves": extract_required_skills(raw_text),
        "keywords": extract_keywords(raw_text),
        "raw_text": raw_text[:5000]  # Store limited raw text for fallback
    }
    return job_ref

# In wizard action handling:
if action_id == "JOB_POSTING_CONFIRM":
    job_ref = parse_job_posting(user_pasted_text)
    meta.job_reference = job_ref  # Store ONCE
    # Now all downstream stages can access: meta.job_reference
```

### Usage Pattern: Reuse Across All AI Tasks

```python
# In work_tailor_run (Step 4)
job_ref = meta.get("job_reference")  # Get already-parsed object
work_context = format_job_for_prompt(job_ref)  # Format for AI
ai_result = call_openai_api(task="work_experience", context=work_context)

# In skills_tailor_run (Step 5b) – SAME OBJECT
job_ref = meta.get("job_reference")  # Reuse same object
skills_context = format_job_for_prompt(job_ref)  # Same formatting
ai_result = call_openai_api(task="it_ai_skills", context=skills_context)

# In tech_ops_tailor_run (Step 5c) – SAME OBJECT
job_ref = meta.get("job_reference")  # Reuse same object
tech_context = format_job_for_prompt(job_ref)  # Same formatting
ai_result = call_openai_api(task="technical_operational_skills", context=tech_context)
```

---

- [ ] Verify all wizard stages fetch inputs from correct source (cv_data vs docx_prefill_unconfirmed)
- [ ] Ensure work_tailoring_notes correctly propagates to both skill-ranking AI tasks
- [ ] Add logging to track data lineage for each section
- [ ] Document API schema for each stage (request/response)
